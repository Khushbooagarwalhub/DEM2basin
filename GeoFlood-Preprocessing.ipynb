{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GeoFlood preprocessing 1m DEM data\n",
    "## Author: Daniel Hardesty Lewis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import needed modules\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import utm\n",
    "from pyproj import CRS\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from rasterio.merge import merge\n",
    "from rasterio.features import shapes\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import glob\n",
    "import sys\n",
    "from threading import Thread\n",
    "from collections import deque\n",
    "import multiprocessing\n",
    "results = []\n",
    "#from multiprocessing import Pool, set_start_method#, Manager\n",
    "import time\n",
    "from itertools import repeat\n",
    "import tblib.pickling_support\n",
    "tblib.pickling_support.install()\n",
    "import logging\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correctly set GDAL_DATA environment variable\n",
    "oldgdal_data = os.environ['GDAL_DATA']\n",
    "os.environ['GDAL_DATA'] = os.path.join(fiona.__path__[0],'gdal_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define input and output file locations\n",
    "\n",
    "## REQUIRED\n",
    "## NHD catchment and flowline vector data\n",
    "## type=str\n",
    "## NHD MR GIS files with layers labelled Flowline and Catchment\n",
    "args.nhd = \"ATX/NFIEGeo_TX-ATX\"\n",
    "\n",
    "## REQUIRED\n",
    "## WBD HUC12 vector data\n",
    "## type=str\n",
    "## WBD HUC12 dataset\n",
    "args.huc12 = \"ATX/WBD_National_GDB-HU12-ATX.shp/WBD_National_GDB-HU12-ATX.shp\"\n",
    "\n",
    "## REQUIRED\n",
    "## Input vector data with single polygon of study area\n",
    "## type=str\n",
    "## Vector GIS file with single polygon of the study area\n",
    "args.shapefile = \"ATX/HUC12.shp/HUC12.shp\"\n",
    "\n",
    "## REQUIRED\n",
    "## Parent directory of TNRIS LIDAR projects\n",
    "## type=str\n",
    "## Parent directory of LIDAR projects\n",
    "args.raster = \"ATX/USGS-ATX.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional arguments\n",
    "\n",
    "## Distance to buffer output raster\n",
    "## type=float\n",
    "## Optional distance to buffer the output raster\n",
    "args.buffer = \"\"\n",
    "\n",
    "## TNRIS LIDAR availability vector data\n",
    "## type=str\n",
    "## TNRIS GIS vector file of available LIDAR data\"\n",
    "args.availability = \"\"\n",
    "\n",
    "## Optional directory for all of the outputs\n",
    "##  (The outputs will be organized by HUC)\n",
    "## type=str\n",
    "## Optional directory for the outputs\n",
    "args.directory = \"\"\n",
    "\n",
    "## Overwrite existing outputs within input area\n",
    "## action='store_true'\n",
    "## Optional flag to overwrite files found in the output directory\n",
    "args.overwrite = \"\"\n",
    "        \n",
    "## Overwrite existing outputs within input area\n",
    "## action='store_true'\n",
    "## Optional flag to overwrite just the flowlines file\n",
    "args.overwrite_flowlines = \"\"\n",
    "        \n",
    "## Overwrite existing outputs within input area\n",
    "## action='store_true'\n",
    "## Optional flag to overwrite just the catchments file\n",
    "args.overwrite_catchments = \"\"\n",
    "        \n",
    "## Overwrite existing outputs within input area\n",
    "## action='store_true'\n",
    "## Optional flag to overwrite just the roughness file\n",
    "args.overwrite_roughnesses = \"\"\n",
    "        \n",
    "## Overwrite existing outputs within input area\n",
    "## action='store_true'\n",
    "## Optional flag to overwrite just the raster file\n",
    "args.overwrite_rasters = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that the required input files have been defined\n",
    "if not args.shapefile:\n",
    "    raise(ValueError('-s --shapefile Input shapefile cutline not specified'))\n",
    "if not args.huc12:\n",
    "    raise(ValueError('-u --huc12 Input HUC12 shapefile not specified'))\n",
    "if not args.nhd:\n",
    "    raise(ValueError('-n --nhd Input NHD geodatabase not specified'))\n",
    "if not args.raster:\n",
    "    raise(ValueError('-r --raster Input raster not specified'))\n",
    "#if not args.availability:\n",
    "#    parser.error('-a --availability Availability shapefile not specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identfy catchments and flowlines to HUCs\n",
    "\n",
    "## Find the HUC12s that intersect with the input polygon\n",
    "shape = gpd.read_file(args.shapefile)\n",
    "shape.drop(columns=['Shape_Leng','Shape_Area'],inplace=True,errors='ignore')\n",
    "shape.rename(columns={'HUC12':'HUC12_shapefile'},inplace=True,errors='ignore')\n",
    "\n",
    "hu12 = gpd.read_file(args.huc12)\n",
    "hu12.drop(columns=['AREAACRES','AREASQKM','SHAPE_Leng','SHAPE_Area'],inplace=True,errors='ignore')\n",
    "\n",
    "hu12shape = gpd.sjoin(hu12,shape.to_crs(hu12.crs),op='intersects',how='inner')\n",
    "hu12shape.drop(columns=['index_right'],inplace=True)\n",
    "\n",
    "## Find the flowlines whose representative points are within these HUC12s\n",
    "flows = gpd.read_file(args.nhd,layer='Flowline')\n",
    "flows.drop(columns=['Shape_Length','Shape_Area','AreaSqKM'],inplace=True,errors='ignore')\n",
    "flows.reset_index(inplace=True)\n",
    "flows.set_index('COMID',inplace=True)\n",
    "flows.sort_index(inplace=True)\n",
    "\n",
    "flows_rep = flows.copy()\n",
    "flows_rep['geometry'] = flows.representative_point()\n",
    "flowshu12shape_rep = gpd.sjoin(flows_rep,hu12shape[['HUC12','geometry']].to_crs(flows_rep.crs),op='intersects',how='inner')\n",
    "flowshu12shape_rep.drop(columns=['index_right'],inplace=True)\n",
    "\n",
    "## Find the catchments corresponding with these flowlines\n",
    "catchs = gpd.read_file(args.nhd,layer='Catchment')\n",
    "catchs.reset_index(inplace=True)\n",
    "catchs.set_index('FEATUREID',inplace=True)\n",
    "catchs.sort_index(inplace=True)\n",
    "\n",
    "catchshu12shape = catchs[catchs.index.isin(flowshu12shape_rep.index)]\n",
    "\n",
    "## Find the flowlines corresponding with these cactchments\n",
    "##  (Note: this line is optional.\n",
    "##  Commenting it out will result in non-COMID-identified flowlines)\n",
    "flowshu12shape = flows[flows.index.isin(catchshu12shape.index)]\n",
    "\n",
    "## Determine which HUC12s each of the flowlines and catchments belong to\n",
    "flowshu12shape['HUC12'] = flowshu12shape_rep.loc[flowshu12shape.index]['HUC12']\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==0,'Roughness'] = .99\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==1,'Roughness'] = .2\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==2,'Roughness'] = .1\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==3,'Roughness'] = .065\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==4,'Roughness'] = .045\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==5,'Roughness'] = .03\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==6,'Roughness'] = .01\n",
    "flowshu12shape.loc[flowshu12shape['StreamOrde']==7,'Roughness'] = .025\n",
    "catchshu12shape['HUC12'] = flowshu12shape.loc[catchshu12shape.index]['HUC12']\n",
    "flowshu12shape = flowshu12shape[flowshu12shape.is_valid]\n",
    "catchshu12shape = catchshu12shape[catchshu12shape.is_valid]\n",
    "catchshu12shape = catchshu12shape[catchshu12shape.index.isin(flowshu12shape.index)]\n",
    "flowshu12shape = flowshu12shape[flowshu12shape.index.isin(catchshu12shape.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer(catchshu12shape):\n",
    "    ## Buffer the catchments for each HUC\n",
    "\n",
    "    def unique(shape):\n",
    "        ## Determine whether the administrative division is within a single UTM\n",
    "\n",
    "        shape.to_crs('epsg:4326',inplace=True)\n",
    "\n",
    "        shape['min'] = list(zip(shape.bounds['miny'],shape.bounds['minx']))\n",
    "        uniq_min = shape['min'].apply(lambda x: utm.latlon_to_zone_number(*x)).unique()\n",
    "        shape.drop(columns=['min'],inplace=True)\n",
    "\n",
    "        shape['max'] = list(zip(shape.bounds['maxy'],shape.bounds['maxx']))\n",
    "        uniq_max = shape['max'].apply(lambda x: utm.latlon_to_zone_number(*x)).unique()\n",
    "        shape.drop(columns=['max'],inplace=True)\n",
    "        #del(shape)\n",
    "\n",
    "        uniq = np.unique(np.append(uniq_min,uniq_max))\n",
    "        #del(uniq_min,uniq_max)\n",
    "        if uniq.shape[0] > 1:\n",
    "            print(\"ERROR: Cross-UTM input shapefile not yet supported.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        return(uniq)\n",
    "\n",
    "    ## Create new HUC12 boundaries from the catchments pertaining to them\n",
    "    hu12catchs = catchshu12shape.dissolve(by='HUC12')\n",
    "\n",
    "    ## Are the catchments all within the same UTM?\n",
    "    uniq = unique(hu12catchs)\n",
    "\n",
    "    ## Buffer the HUC12 catchments\n",
    "    if hu12catchs.crs.datum.name=='World Geodetic System 1984':\n",
    "        #crs = CRS(proj='utm', zone=uniq[0], datum='WGS84')\n",
    "        if uniq[0]==13:\n",
    "            crs = 'epsg:32613'\n",
    "        elif uniq[0]==14:\n",
    "            crs = 'epsg:32614'\n",
    "        elif uniq[0]==15:\n",
    "            crs = 'epsg:32615'\n",
    "        else:\n",
    "            print(\"ERROR: UTMs outside of 13-15 not yet supported.\")\n",
    "            sys.exit(0)\n",
    "    elif hu12catchs.crs.datum.name=='North American Datum 1983' or hu12catchs.crs.datum.name=='D_NORTH_AMERICAN_1983' or hu12catchs.crs.datum.name=='NAD83 (National Spatial Reference System 2011)':\n",
    "        #crs = CRS(proj='utm', zone=uniq[0], datum='NAD83')\n",
    "        if uniq[0]==13:\n",
    "            crs = 'epsg:6342'\n",
    "        elif uniq[0]==14:\n",
    "            crs = 'epsg:6343'\n",
    "        elif uniq[0]==15:\n",
    "            crs = 'epsg:6344'\n",
    "        else:\n",
    "            print(\"ERROR: UTMs outside of 13-15 not yet supported.\")\n",
    "            sys.exit(0)\n",
    "    else:\n",
    "        print(\"ERROR: Non-WGS/NAD datum not yet supported\")\n",
    "        sys.exit(0)\n",
    "    if args.buffer:\n",
    "        hu12catchs['geometry'] = hu12catchs.to_crs(crs).buffer(args.buffer)\n",
    "    else:\n",
    "        hu12catchs['geometry'] = hu12catchs.to_crs(crs).buffer(500.)\n",
    "    hu12catchs.crs = crs\n",
    "\n",
    "    ## Are the buffered catchments all within the same UTM?\n",
    "    unique(hu12catchs)\n",
    "\n",
    "    return(crs,hu12catchs)\n",
    "\n",
    "def available(hu12catchs):\n",
    "    ## Identify each DEM tile file for our study area\n",
    "\n",
    "    ## Find the DEM tiles that intersect with these buffered HUC12 catchments\n",
    "    #availibility = 'data/TNRIS-LIDAR-Availability-20191213.shp/TNRIS-LIDAR-Availability-20191213.shp'\n",
    "    avail = gpd.read_file(args.availability)\n",
    "    avail_hu12catchs = gpd.sjoin(avail,hu12catchs.to_crs(avail.crs),op='intersects',how='inner')\n",
    "    #del(avail,hu12catchs)\n",
    "    ## Construct an exact path for each DEM tile\n",
    "    fnexts = ['.dem','.img']\n",
    "    for fnext in fnexts:\n",
    "        avail_hu12catchs['demname'] = avail_hu12catchs['demname'].str.replace(fnext+'$','')\n",
    "    #del(fnext)\n",
    "    for dirname in avail_hu12catchs['dirname'].unique():\n",
    "        stampede2names = []\n",
    "        #raster = '/scratch/projects/tnris/tnris-lidardata'\n",
    "        basename = os.path.join(args.raster,dirname,'dem')+os.sep\n",
    "        for fnext in fnexts:\n",
    "            avail_hu12catchs['demname'] = avail_hu12catchs['demname'].str.replace(fnext+'$','')\n",
    "            stampede2names.extend(glob.glob(basename+'*'+fnext))\n",
    "        #del(fnext)\n",
    "        direxts = set([os.path.splitext(os.path.basename(name))[1] for name in stampede2names])\n",
    "        ## If more than one vector image extension found in a DEM project,\n",
    "        ##  then figure out each file's extension individually\n",
    "        ## TODO: Test this against stratmap-2013-50cm-ellis-henderson-hill-johnson-navarro\n",
    "        if len(direxts) > 1:\n",
    "            for demname in avail_hu12catchs.loc[avail_hu12catchs['dirname']==dirname,'demname'].unique():\n",
    "                truth_dirname = avail_hu12catchs['dirname']==dirname\n",
    "                truth_demname = avail_hu12catchs['demname']==demname\n",
    "                truth = np.logical_and(truth_dirname,truth_demname)\n",
    "                #del(truth_dirname,truth_demname)\n",
    "                for fnext in fnexts:\n",
    "                    stampede2name = avail_hu12catchs.loc[truth,'demname'].apply(lambda x: os.path.join(basename,x+fnext))\n",
    "                    if glob.glob(stampede2name.iloc[0]):\n",
    "                        break\n",
    "                    #else:\n",
    "                        #del(stampede2name)\n",
    "                    #del(fnext)\n",
    "                avail_hu12catchs.loc[truth,'stampede2name'] = stampede2name\n",
    "                #del(truth,demname)\n",
    "        ## Else do all the files in a DEM project at once\n",
    "        elif len(direxts) == 1:\n",
    "            stampede2name = avail_hu12catchs.loc[avail_hu12catchs['dirname']==dirname,'demname'].apply(lambda x: os.path.join(basename,x+list(direxts)[0]))\n",
    "            stampede2name.drop_duplicates(inplace=True)\n",
    "            p = Path(basename)\n",
    "            for subp in p.rglob('*'):\n",
    "                if len(stampede2name[stampede2name.str.lower()==str(subp).lower()].index)>0:\n",
    "                    stampede2name.loc[stampede2name[stampede2name.str.lower()==subp.as_posix().lower()].index[0]] = subp.as_posix()\n",
    "            stampede2name = stampede2name[stampede2name.isin([subp.as_posix() for subp in list(p.rglob('*'))])]\n",
    "            avail_hu12catchs.loc[avail_hu12catchs['dirname']==dirname,'stampede2name'] = stampede2name\n",
    "        else:\n",
    "            continue\n",
    "        #del(stampede2names,stampede2name,basename,direxts,dirname)\n",
    "    #del(fnexts)\n",
    "    avail_hu12catchs.dropna(subset=['stampede2name'],inplace=True)\n",
    "    avail_hu12catchs_grouped = avail_hu12catchs.groupby('index_right')\n",
    "    #del(avail_hu12catchs)\n",
    "\n",
    "    return(avail_hu12catchs_grouped)\n",
    "\n",
    "class ExceptionWrapper(object):\n",
    "\n",
    "    def __init__(self, ee):\n",
    "        self.ee = ee\n",
    "        __, __, self.tb = sys.exc_info()\n",
    "\n",
    "    def re_raise(self):\n",
    "        raise(self.ee.with_traceback(self.tb))\n",
    "\n",
    "#@profile\n",
    "def output_files(arguments):\n",
    "#def output(flow_key,flowshu12shape,catchshu12shape,hu12catchs,avail_hu12catchs_group,args,prefix,dst_crs,mem_estimates):\n",
    "    ## Output catchments, flowlines, roughnesses, and rasters\n",
    "\n",
    "    try:\n",
    "\n",
    "        def output_nhd(flowshu12shape,catchshu12shape,hu):\n",
    "            ## For each HUC, write the flowlines, catchments, and roughnesses corresponding to it\n",
    "\n",
    "            #print('INSIDE OUTPUT_NHD\\t',arguments[0])\n",
    "            #print('INSIDE OUTPUT_NHD\\t',flows_key)\n",
    "            #sys.stdout.flush()\n",
    "            out_path = os.path.join(subdirectory, 'Flowlines.shp')\n",
    "            my_file = Path(out_path)\n",
    "            #if my_file.is_file() and not arguments[1].args.overwrite and not arguments[1].args.overwrite_flowlines:\n",
    "            if my_file.is_file() and not arguments[5].overwrite and not arguments[5].overwrite_flowlines:\n",
    "            #if my_file.is_file() and not args.overwrite and not args.overwrite_flowlines:\n",
    "                #del(my_file)\n",
    "                pass\n",
    "            else:\n",
    "                my_file.unlink(missing_ok=True)\n",
    "                #del(my_file)\n",
    "                #flowshu12shape[flowshu12shape['HUC12']==hu].reset_index().to_file(out_path)\n",
    "                #print('OUTPUTTING FLOWS\\t',arguments[0])\n",
    "                #print('OUTPUTTING FLOWS\\t',flows_key)\n",
    "                #sys.stdout.flush()\n",
    "                flowshu12shape.reset_index().to_file(out_path)\n",
    "            #del(out_path)\n",
    "\n",
    "            out_path = os.path.join(subdirectory, 'Roughness.csv')\n",
    "            my_file = Path(out_path)\n",
    "            #if my_file.is_file() and not arguments[1].args.overwrite and not arguments[1].args.overwrite_roughnesses:\n",
    "            if my_file.is_file() and not arguments[5].overwrite and not arguments[5].overwrite_roughnesses:\n",
    "            #if my_file.is_file() and not args.overwrite and not args.overwrite_roughnesses:\n",
    "                #del(my_file)\n",
    "                pass\n",
    "            else:\n",
    "                my_file.unlink(missing_ok=True)\n",
    "                #del(my_file)\n",
    "                with open(out_path, 'w', newline='') as outcsv:\n",
    "                    #print('OUTPUTTING ROUGHS\\t',arguments[0])\n",
    "                    #print('OUTPUTTING ROUGHS\\t',flows_key)\n",
    "                    #sys.stdout.flush()\n",
    "                    writer = csv.writer(outcsv)\n",
    "                    writer.writerow(['COMID','StreamOrde','Roughness'])\n",
    "                    for comid in np.sort(flowshu12shape.index.unique()):\n",
    "                        writer.writerow([comid,flowshu12shape.loc[comid,'StreamOrde'],flowshu12shape.loc[comid,'Roughness']])\n",
    "                    #del(comid,writer)\n",
    "            #del(out_path,flowshu12shape)\n",
    "\n",
    "            out_path = os.path.join(subdirectory, 'Catchments.shp')\n",
    "            my_file = Path(out_path)\n",
    "            #if my_file.is_file() and not arguments[1].args.overwrite and not arguments[1].args.overwrite_catchments:\n",
    "            if my_file.is_file() and not arguments[5].overwrite and not arguments[5].overwrite_catchments:\n",
    "            #if my_file.is_file() and not args.overwrite and not args.overwrite_catchments:\n",
    "                #del(my_file)\n",
    "                pass\n",
    "            else:\n",
    "                my_file.unlink(missing_ok=True)\n",
    "                #del(my_file)\n",
    "                #catchshu12shape[catchshu12shape['HUC12']==hu].reset_index().to_file(out_path)\n",
    "                #print('OUTPUTTING CATCHS\\t',arguments[0])\n",
    "                #print('OUTPUTTING CATCHS\\t',flows_key)\n",
    "                #sys.stdout.flush()\n",
    "                catchshu12shape.reset_index().to_file(out_path)\n",
    "            #del(out_path,catchshu12shape)\n",
    "\n",
    "        def get_mosaic(avail_hu12catchs_group,hu,break_hu,dst_crs):\n",
    "            ## Get mosaic of DEMs for each HUC\n",
    "\n",
    "            def append_check(src_files_to_mosaic,var,subdirectory,hu):\n",
    "                ## Check each raster's resolution in this HUC\n",
    "\n",
    "                if any(np.float16(i) > 1. for i in var.res):\n",
    "                    out_path = os.path.join(subdirectory, \"gt1m.err\")\n",
    "                    Path(out_path).touch()\n",
    "                    print('WARNING: >1m raster input for HUC12: '+str(hu))\n",
    "                    sys.stdout.flush()\n",
    "                    #del(out_path)\n",
    "                else:\n",
    "                    src_res_min_to_mosaic.append(min(var.res))\n",
    "                    src_res_max_to_mosaic.append(min(var.res))\n",
    "                    src_x_to_mosaic.append(var.res[0])\n",
    "                    src_y_to_mosaic.append(var.res[1])\n",
    "                    src_files_to_mosaic.append(var)\n",
    "\n",
    "                return(src_files_to_mosaic,src_res_min_to_mosaic,src_res_max_to_mosaic,src_x_to_mosaic,src_y_to_mosaic)\n",
    "\n",
    "            ## Reproject the mosaic to DEM tiles pertaining to each HUC\n",
    "            #print('INSIDE GET MOSAIC\\t',arguments[0])\n",
    "            #print('INSIDE GET MOSAIC\\t',flows_key)\n",
    "            #sys.stdout.flush()\n",
    "            dem_fps = list(avail_hu12catchs_group['stampede2name'])\n",
    "            src_files_to_mosaic = []\n",
    "            src_res_min_to_mosaic = []\n",
    "            src_res_max_to_mosaic = []\n",
    "            src_x_to_mosaic = []\n",
    "            src_y_to_mosaic = []\n",
    "            memfile = {}\n",
    "            for fp in dem_fps:\n",
    "                memfile[fp] = MemoryFile()\n",
    "                #del(fp)\n",
    "            for fp in dem_fps:\n",
    "                with rasterio.open(fp) as src:\n",
    "                    transform, width, height = calculate_default_transform(\n",
    "                        src.crs,\n",
    "                        dst_crs,\n",
    "                        src.width,\n",
    "                        src.height,\n",
    "                        *src.bounds\n",
    "                    )\n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update({\n",
    "                        'crs': dst_crs,\n",
    "                        'transform': transform,\n",
    "                        'width': width,\n",
    "                        'height': height\n",
    "                    })\n",
    "                    #del(width,height)\n",
    "\n",
    "                    ## Don't do an expensive reprojection if projection already correct\n",
    "                    if src.meta==out_meta:\n",
    "                        #del(fp,transform)\n",
    "                        src_files_to_mosaic, src_res_min_to_mosaic, src_res_max_to_mosaic, src_x_to_mosaic, src_y_to_mosaic = append_check(src_files_to_mosaic,src,subdirectory,hu)\n",
    "                    else:\n",
    "                        dst = memfile[fp].open(**out_meta)\n",
    "                        #del(fp,out_meta)\n",
    "                        for i in range(1, src.count + 1):\n",
    "                            reproject(\n",
    "                                source=rasterio.band(src, i),\n",
    "                                destination=rasterio.band(dst, i),\n",
    "                                src_transform=src.transform,\n",
    "                                src_crs=src.crs,\n",
    "                                dst_transform=dst.transform,\n",
    "                                dst_crs=dst.crs,\n",
    "                                resampling=Resampling.nearest\n",
    "                            )\n",
    "                            #del(i)\n",
    "                        #del(transform)\n",
    "                        src_files_to_mosaic, src_res_min_to_mosaic, src_res_max_to_mosaic, src_x_to_mosaic, src_y_to_mosaic = append_check(src_files_to_mosaic,dst,subdirectory,hu)\n",
    "                        #del(dst)\n",
    "            #del(dem_fps)\n",
    "\n",
    "            if len(src_files_to_mosaic) == 0:\n",
    "\n",
    "                out_path = os.path.join(subdirectory, \"allGT1m.err\")\n",
    "                Path(out_path).touch()\n",
    "                print('WARNING: Found no <=1m raster input data for HUC12: '+str(hu))\n",
    "                sys.stdout.flush()\n",
    "                #del(out_path)\n",
    "\n",
    "                break_hu = True\n",
    "                mosaic_tuple = ()\n",
    "                return(break_hu,mosaic_tuple)\n",
    "\n",
    "            else:\n",
    "\n",
    "                src_files_to_mosaic = pd.DataFrame(data={\n",
    "                    'Files':src_files_to_mosaic,\n",
    "                    'min(resolution)':src_res_min_to_mosaic,\n",
    "                    'max(resolution)':src_res_max_to_mosaic\n",
    "                })\n",
    "                #del(src_res_min_to_mosaic,src_res_max_to_mosaic)\n",
    "                src_files_to_mosaic.sort_values(by=['min(resolution)','max(resolution)'],inplace=True)\n",
    "                mosaic, out_trans = merge(list(src_files_to_mosaic['Files']),res=(max(src_x_to_mosaic),max(src_y_to_mosaic)))\n",
    "                #del(src_x_to_mosaic,src_y_to_mosaic)\n",
    "                for src in src_files_to_mosaic['Files']:\n",
    "                    src.close()\n",
    "                #del(src_files_to_mosaic)\n",
    "                out_meta = src.meta.copy()\n",
    "                #del(src)\n",
    "                out_meta.update({\n",
    "                    \"driver\": 'GTiff',\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans,\n",
    "                    \"crs\": dst_crs\n",
    "                })\n",
    "                #del(out_trans)\n",
    "                for keyvalue in memfile.items():\n",
    "                    keyvalue[1].close()\n",
    "                    #del(keyvalue)\n",
    "                #del(memfile)\n",
    "\n",
    "                mosaic_tuple = (mosaic,out_meta)\n",
    "                return(break_hu,mosaic_tuple)\n",
    "\n",
    "        def output_raster(hu_buff_geom,mosaic,out_meta,path_elevation):\n",
    "            ## Crop and output the mosaic to the buffered catchments of each HUC\n",
    "\n",
    "            #print('INSIDE OUTPUT RASTER\\t',arguments[0])\n",
    "            #print('INSIDE OUTPUT RASTER\\t',flows_key)\n",
    "            #sys.stdout.flush()\n",
    "            with MemoryFile() as memfile:\n",
    "                with memfile.open(**out_meta) as dataset:\n",
    "                    dataset.write(mosaic)\n",
    "                with memfile.open(**out_meta) as dataset:\n",
    "                    out_image, out_trans = rasterio.mask.mask(dataset,hu_buff_geom,crop=True)\n",
    "            #del(hu_buff_geom)\n",
    "\n",
    "            out_meta.update({\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\":out_image.shape[2],\n",
    "                \"transform\": out_trans\n",
    "            })\n",
    "            #del(out_trans)\n",
    "\n",
    "            with rasterio.open(path_elevation,\"w\",**out_meta) as dst:\n",
    "                dst.write(out_image)\n",
    "            #del(path_elevation,out_meta,out_image,dst)\n",
    "\n",
    "        #print('INSIDE\\t',arguments[0])\n",
    "        #print('INSIDE\\t',flows_key)\n",
    "        #sys.stdout.flush()\n",
    "        #subdirectory = os.path.join(arguments[1].args.directory, arguments[1].prefix+'-'+str(arguments[0]))\n",
    "        subdirectory = os.path.join(arguments[5].directory, arguments[6]+'-'+str(arguments[0]))\n",
    "        #subdirectory = os.path.join(args.directory, prefix+'-'+str(flow_key))\n",
    "        Path(subdirectory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        path_notime = os.path.join(subdirectory, \"jobNoTimeLeftWhileProcessing.err\")\n",
    "        Path(path_notime).touch()\n",
    "\n",
    "        path_gt1m = os.path.join(subdirectory, \"allGT1m.err\")\n",
    "        file_gt1m = Path(path_gt1m)\n",
    "        path_enclose = os.path.join(subdirectory, \"rasterDataDoesNotEnclose.err\")\n",
    "        file_enclose = Path(path_enclose)\n",
    "\n",
    "        if file_gt1m.is_file() or file_enclose.is_file():\n",
    "\n",
    "            #del(file_gt1m,path_gt1m,file_enclose,path_enclose)\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            #print('GOING IN OUTPUT_NHD\\t',arguments[0])\n",
    "            #print('GOING IN OUTPUT_NHD\\t',flows_key)\n",
    "            #sys.stdout.flush()\n",
    "            #output_nhd(arguments[1].flowshu12shape,arguments[1].catchshu12shape,arguments[0])\n",
    "            output_nhd(arguments[1],arguments[2],arguments[0])\n",
    "            #output_nhd(flowshu12shape,catchshu12shape,flow_key)\n",
    "\n",
    "            path_elevation = os.path.join(subdirectory, 'Elevation.tif')\n",
    "            file_elevation = Path(path_elevation)\n",
    "            #if file_elevation.is_file() and not arguments[1].args.overwrite and not arguments[1].args.overwrite_rasters:\n",
    "            if file_elevation.is_file() and not arguments[5].overwrite and not arguments[5].overwrite_rasters:\n",
    "            #if file_elevation.is_file() and not args.overwrite and not args.overwrite_rasters:\n",
    "\n",
    "                #del(file_elevation,path_elevation)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                file_elevation.unlink(missing_ok=True)\n",
    "                #del(file_elevation)\n",
    "\n",
    "                #avail_hu12catchs_group = arguments[1].avail_hu12catchs_grouped.get_group(arguments[0])\n",
    "                break_hu = False\n",
    "\n",
    "                #print('GOING IN GET MOSAIC\\t',arguments[0])\n",
    "                #print('GOING IN GET MOSAIC\\t',flows_key)\n",
    "                #sys.stdout.flush()\n",
    "                #break_hu, mosaic_tuple = get_mosaic(avail_hu12catchs_group,arguments[0],break_hu,arguments[1].dst_crs)\n",
    "                break_hu, mosaic_tuple = get_mosaic(arguments[4],arguments[0],break_hu,arguments[7])\n",
    "                #break_hu, mosaic_tuple = get_mosaic(avail_hu12catchs_group,flow_key,break_hu,dst_crs)\n",
    "\n",
    "                if break_hu!=True:\n",
    "\n",
    "                    with rasterio.Env():\n",
    "                        results = ({\n",
    "                            'properties': {\n",
    "                                'Elevation': v\n",
    "                            },\n",
    "                            'geometry': s\n",
    "                        }\n",
    "                        for i, (s, v) in enumerate(shapes(\n",
    "                            (mosaic_tuple[0]==mosaic_tuple[1]['nodata']).astype(np.int16),\n",
    "                            mask=mosaic_tuple[0]!=mosaic_tuple[1]['nodata'],\n",
    "                            transform=mosaic_tuple[1]['transform']\n",
    "                        )))\n",
    "                    geoms = list(results)\n",
    "                    raster = gpd.GeoDataFrame.from_features(geoms,crs=mosaic_tuple[1]['crs'])\n",
    "\n",
    "                    #hu_buff = arguments[1].hu12catchs.loc[[arguments[0]]].drop(columns=['index_left','index_right'],errors='ignore').to_crs(mosaic_tuple[1]['crs'])\n",
    "                    hu_buff = arguments[3].to_crs(mosaic_tuple[1]['crs'])\n",
    "                    #hu_buff = hu12catchs.to_crs(mosaic_tuple[1]['crs'])\n",
    "                    hu_buff_geom = list(hu_buff['geometry'])\n",
    "\n",
    "                    if len(gpd.sjoin(hu_buff,raster,op='within',how='inner').index) == 0:\n",
    "                        out_path = os.path.join(subdirectory, \"rasterDataDoesNotEnclose.err\")\n",
    "                        Path(out_path).touch()\n",
    "                        print('WARNING: <=1m raster input data does not enclose HUC12: '+str(arguments[0]))\n",
    "                        #print('WARNING: <=1m raster input data does not enclose HUC12: '+str(flow_key))\n",
    "                        sys.stdout.flush()\n",
    "                        #del(out_path)\n",
    "                    else:\n",
    "                        #print('GOING IN OUTPUT RASTER\\t',arguments[0])\n",
    "                        #print('GOING IN OUTPUT RASTER\\t',flows_key)\n",
    "                        output_raster(hu_buff_geom,mosaic_tuple[0],mosaic_tuple[1],path_elevation)\n",
    "\n",
    "        #print('ZEROING MEM_ESTIMATE\\t',arguments[0])\n",
    "        #print('ZEROING MEM_ESTIMATE\\t',flows_key)\n",
    "        #sys.stdout.flush()\n",
    "        #mem_estimates[flows_key] = 0.\n",
    "        Path(path_notime).unlink()\n",
    "        #del(path_notime,subdirectory)\n",
    "\n",
    "    except OSError as e:\n",
    "        Path(path_notime).unlink()\n",
    "        out_path = os.path.join(subdirectory, \"OS.err\")\n",
    "        Path(out_path).touch()\n",
    "        with open(out_path, 'w') as f:\n",
    "            #f.write(\"{}\".format(e))\n",
    "            f.write(str(e))\n",
    "        print('[ERROR] OSError on HUC12: '+str(arguments[0]))\n",
    "        print(e)\n",
    "        sys.stdout.flush()\n",
    "        #if arguments[1].args.log:\n",
    "        if arguments[5].log:\n",
    "        #if args.log:\n",
    "            logging.debug('[ERROR] OSError on HUC '+str(arguments[0]))\n",
    "            #logging.debug('HUC '+str(flow_key))\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        #if arguments[1].args.log:\n",
    "        if arguments[5].log:\n",
    "        #if args.log:\n",
    "            logging.debug('[EXCEPTION] on HUC '+str(arguments[0]))\n",
    "            #logging.debug('HUC '+str(flow_key))\n",
    "        return(ExceptionWrapper(e))\n",
    "\n",
    "    #except:\n",
    "    #    print(sys.exc_info()[0])\n",
    "    #    raise\n",
    "\n",
    "def collect_results(result):\n",
    "    results.append(result)\n",
    "\n",
    "class TaskProcessor(Thread):\n",
    "    \"\"\"\n",
    "    Processor class which monitors memory usage for running tasks (processes).\n",
    "    Suspends execution for tasks surpassing `max_b` and completes them one\n",
    "    by one, after behaving tasks have finished.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_cores, max_b, tasks):\n",
    "        super().__init__()\n",
    "        self.n_cores = n_cores\n",
    "        self.max_b = max_b\n",
    "        self.tasks = deque(tasks)\n",
    "\n",
    "        self._running_tasks = []\n",
    "        self._suspended_tasks = []\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main-function in new thread.\"\"\"\n",
    "        self._update_running_tasks()\n",
    "        self._monitor_running_tasks()\n",
    "        self._update_suspended_tasks()\n",
    "        self._monitor_suspended_tasks()\n",
    "#        self._process_suspended_tasks()\n",
    "\n",
    "    def _update_running_tasks(self):\n",
    "        \"\"\"Start new tasks if we have less running tasks than cores.\"\"\"\n",
    "        while len(self._running_tasks) < self.n_cores and len(self.tasks) > 0:\n",
    "            p = self.tasks.popleft()\n",
    "            gc.collect()\n",
    "            p.start()\n",
    "            # for further process-management we here just need the\n",
    "            # psutil.Process wrapper\n",
    "            self._running_tasks.append(psutil.Process(pid=p.pid))\n",
    "            print(f'Started process: {self._running_tasks[-1]}')\n",
    "\n",
    "    def _monitor_running_tasks(self):\n",
    "        \"\"\"\n",
    "        Monitor running tasks. Replace completed tasks and suspend tasks\n",
    "        which exceed the memory threshold `self.max_b`.\n",
    "        \"\"\"\n",
    "        # loop while we have running or non-started tasks\n",
    "        while self._running_tasks or self.tasks:\n",
    "            multiprocessing.active_children() # Joins all finished processes.\n",
    "            # Without it, p.is_running() below on Unix would not return\n",
    "            # `False` for finished processes.\n",
    "            self._update_running_tasks()\n",
    "            actual_tasks = self._running_tasks.copy()\n",
    "\n",
    "            for p in actual_tasks:\n",
    "                if not p.is_running():  # process has finished\n",
    "                    self._running_tasks.remove(p)\n",
    "                    print(f'Removed finished process: {p}')\n",
    "                else:\n",
    "                    if p.memory_info().rss > psutil.virtual_memory().available - self.max_b*.1:\n",
    "                        p.suspend()\n",
    "                        self._running_tasks.remove(p)\n",
    "                        self._suspended_tasks.append(p)\n",
    "                        print(f'Suspended process: {p}')\n",
    "\n",
    "            time.sleep(.005)\n",
    "\n",
    "    def _update_suspended_tasks(self):\n",
    "        \"\"\"Start new tasks if we have less running tasks than cores.\"\"\"\n",
    "        while len(self._running_tasks) < self.n_cores and len(self._suspended_tasks) > 0:\n",
    "            p = self._suspended_tasks.popleft()\n",
    "            gc.collect()\n",
    "            p.resume()\n",
    "            # for further process-management we here just need the\n",
    "            # psutil.Process wrapper\n",
    "            self._running_tasks.append(p)\n",
    "            print(f'Resumed process: {self._running_tasks[-1]}')\n",
    "\n",
    "    def _monitor_suspended_tasks(self):\n",
    "        \"\"\"\n",
    "        Monitor running tasks. Replace completed tasks and suspend tasks\n",
    "        which exceed the memory threshold `self.max_b`.\n",
    "        \"\"\"\n",
    "        # loop while we have running or non-started tasks\n",
    "        while self._running_tasks or self._suspended_tasks:\n",
    "            multiprocessing.active_children() # Joins all finished processes.\n",
    "            # Without it, p.is_running() below on Unix would not return\n",
    "            # `False` for finished processes.\n",
    "            self._update_suspended_tasks()\n",
    "            actual_tasks = self._running_tasks.copy()\n",
    "\n",
    "            for p in actual_tasks:\n",
    "                if not p.is_running():  # process has finished\n",
    "                    self._running_tasks.remove(p)\n",
    "                    print(f'Removed finished process: {p}')\n",
    "                else:\n",
    "                    if p.memory_info().rss > psutil.virtual_memory().available - self.max_b*.1:\n",
    "                        p.suspend()\n",
    "                        self._running_tasks.remove(p)\n",
    "                        self._suspended_tasks.append(p)\n",
    "                        print(f'Suspended process: {p}')\n",
    "\n",
    "            time.sleep(.005)\n",
    "\n",
    "def main():\n",
    "\n",
    "    if not args.restart or no_restart_file:\n",
    "\n",
    "        flowshu12shape,catchshu12shape = flows_catchs()\n",
    "        crs,hu12catchs = buffer(catchshu12shape)\n",
    "        flowshu12shape.to_crs(crs,inplace=True)\n",
    "        catchshu12shape.to_crs(crs,inplace=True)\n",
    "        avail_hu12catchs_grouped = available(hu12catchs)\n",
    "    \n",
    "        ## Ensure lists share the same HUC12s\n",
    "        flows_keys = np.sort(list(set(avail_hu12catchs_grouped.groups.keys()).intersection(flowshu12shape['HUC12'])))\n",
    "        flows_keys = np.sort(list(set(flows_keys).intersection(catchshu12shape['HUC12'])))\n",
    "        flows_keys = np.sort(list(set(flows_keys).intersection(hu12catchs.index)))\n",
    "    \n",
    "        ## Divide into lists per HUC12\n",
    "        flowshu12shape = list(dict(tuple(flowshu12shape[flowshu12shape['HUC12'].isin(flows_keys)].sort_values('HUC12').groupby('HUC12'))).values())\n",
    "        catchshu12shape = list(dict(tuple(catchshu12shape[catchshu12shape['HUC12'].isin(flows_keys)].sort_values('HUC12').groupby('HUC12'))).values())\n",
    "        hu12catchs.drop(columns=['index_left','index_right'],errors='ignore',inplace=True)\n",
    "        hu12catchs = list(dict(tuple(hu12catchs[hu12catchs.index.isin(flows_keys)].sort_index().groupby('HUC12'))).values())\n",
    "        avail_hu12catchs_grouped = list({k:dict(tuple(avail_hu12catchs_grouped))[k] for k in flows_keys}.values())\n",
    "    \n",
    "        ## Sort lists by estimated memory usage\n",
    "        mem_estimates = {}\n",
    "        for i in range(len(avail_hu12catchs_grouped)):\n",
    "            mem_estimates[i] = avail_hu12catchs_grouped[i]['stampede2name'].apply(lambda x: Path(x).stat().st_size).sum()\n",
    "        mem_estimates = {k: v for k, v in sorted(mem_estimates.items(), key=lambda item: item[1])}\n",
    "        mem_estimates = {k: v for k, v in mem_estimates.items() if v < psutil.virtual_memory().total}\n",
    "        flows_keys = [flows_keys[i] for i in mem_estimates.keys()]\n",
    "        flowshu12shape = [flowshu12shape[i] for i in mem_estimates.keys()]\n",
    "        catchshu12shape = [catchshu12shape[i] for i in mem_estimates.keys()]\n",
    "        hu12catchs = [hu12catchs[i] for i in mem_estimates.keys()]\n",
    "        avail_hu12catchs_grouped = [avail_hu12catchs_grouped[i] for i in mem_estimates.keys()]\n",
    "\n",
    "    if args.restart and no_restart_file:\n",
    "        with open(args.restart, 'wb') as output:\n",
    "            pickle.dump(\n",
    "                [flows_keys, flowshu12shape, catchshu12shape, hu12catchs, avail_hu12catchs_grouped, crs],\n",
    "                output,\n",
    "                pickle.HIGHEST_PROTOCOL\n",
    "            )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    prefix = os.path.splitext(os.path.basename(args.shapefile))[0]\n",
    "    dst_crs = rasterio.crs.CRS.from_dict(init=crs)\n",
    "\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "\n",
    "    MAX_B = psutil.virtual_memory().total\n",
    "    N_CORES = multiprocessing.cpu_count()-1\n",
    "\n",
    "    arguments = zip(flows_keys, flowshu12shape, catchshu12shape, hu12catchs, avail_hu12catchs_grouped, repeat(args), repeat(prefix), repeat(dst_crs))\n",
    "    tasks = [multiprocessing.Process(target=output_files, args=(argument,)) for argument in arguments]\n",
    "    pool = TaskProcessor(n_cores=N_CORES, max_b=MAX_B, tasks = tasks)\n",
    "    pool.start()\n",
    "    pool.join()\n",
    "\n",
    "    print(\"All catchments, flowlines, roughnesses, and rasters created for each HUC\")\n",
    "    print(\"Time spent with \", N_CORES, \" threads in milliseconds\")\n",
    "    print(\"-----\", int((time.time()-start_time)*1000), \"-----\")\n",
    "\n",
    "    os.environ['GDAL_DATA'] = oldgdal_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
